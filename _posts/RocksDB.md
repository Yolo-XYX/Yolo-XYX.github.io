###### 第一章：背景

```java
1.读多写少、写多读少
对于读多写少的场景比较常见，比如MySQL就是读多写少，底层使用的是InnoDB存储引擎，使用B+树实现的；但是对于像日志这种需要经常写，很少读的场景就不适合，因此需要新的数据库就是RocksDB，他的底层采用的是lsm tree（log structured merge tree）这一数据结构，也就是日志归并树的数据结构实现的
 
2.原地写、追加写
原地写：倘若要针对一组 kv 数据执行更新操作，首先要找到 kv 老数据的所在位置，再在其基础之上执行进行更新. 这个过程涉及到磁盘的随机 IO，因此性能较差.与之相对，在执行读操作时，可以根据 k 寻找到 kv 数据所在位置并直接拿到查询结果，因此读操作效率相对较高. 且具有着不错的空间利用率.
追加写：追加写类型的写操作中，无须区分本次写操作是插入还是更新，而是选择将 kv 对以追加的形式直接插入到文件的末尾位置. 因此不涉及磁盘的随机 IO，只需要执行顺序 IO 操作，在写流程中的执行性能相较于原地写而言有较大的提升.
//注：所谓的追加写就是把相关数据直接追加到最后面，这个时候前面的就失效了，这样虽然写的快了，但是也带来许多问题，比如数据冗余问题，前面存储了旧数据后面也存储了新数据；同时对于读操作只能从末尾开始读，这就导致需要耗费很长的时间，也就是o(n)时间复杂度
```

![1](D:\work\项目\KV\图片\1.png)

```java
3.RocksDB
rocksdb 是 Facebook 使用纯 C++ 研发的嵌入式 kv 存储引擎，依赖的核心数据结构是 lsm tree 存储引擎，底层正是基于追加写策略实现，适用于写多读少的使用场景.
```

###### 第二章：lsm tree

```java
1.追加写
由于由于RocksDB面向的是写多读少的场景，因此要尽可能提高到磁盘上的性能，因此这里采用追加写的方式进行实现。
但是追加写存在下面两个问题：数据冗余、读性能低
①数据冗余（空间浪费）
顺序写无视 k 此前的存在状况，简单粗暴地追加的方式完成数据的写入或者更新操作，因此不可避免地存在一组 kv 对对应多份冗余记录的情况. 并且，除了最后一笔记录之外，此前多笔老数据都属于无用的冗余数据. 考虑一个最极端的场景，在对顺序写策略不执行任何优化改进的前提下，只需要一笔kv对数据，采用无限次写操作即可打满磁盘空间.

②读性能低
顺序写模式下，执行一次查询操作需要反向追溯，直到找到第一笔满足条件的 kv 数据记录才能返回. 因此，其最坏的时间复杂度是线性的 O(N)，显然无法满足使用诉求.

2.数据合并
由于数据在存储的过程中存在冗余问题，我们可以对数据进行合并，在合并的过程中把冗余的数据去掉，这样就可以降低冗余度，具体如何实现后面会说
//注：这里的数据合并其实也是压缩的一部分
    
3.文件分块
倘若我们把一个大文件拆分为一系列的小文件（table），每次写操作只会追加到最新的 new table 中，而异步的压缩合并操作则只面向于老文件 old table，这样两个流程之间就能够实现解耦，写操作不再有陷入阻塞的风险，同时压缩操作和写操作之间也能存在清晰明确的界限.
//注：这里也就是我们对磁盘中存储的数据进行分块存储，这样就可以进行相关的追加操作
    
4.数据有序存储
如果数据没有进行有序存储，那么在查询数据的时候就需要较高的时间复杂度，这里我们可以采取的策略是，在组织每个 table 内的数据时，事先根据 k 进行数据排序，那么在后续的查询环节中，我们就无需承受线性遍历的代价，而是能通过二分的方式在对数级的时间复杂度下获得我们想要的结果.
//注：如果我们要求table中的数据有序，那又和追加存储冲突了，因此需要使用内存+磁盘进行解决
```

![2](D:\work\项目\KV\图片\2.png)

```java
5.内存+磁盘
• memtable 在内存中缓存
• memtable 本身基于 k 进行有序存储
• memtable 中的写操作统一采用就地写
• memtable 是用户写操作的唯一入口
• memtable 数据量达到阈值后溢写到磁盘，成为 disktable
//注：但是上面存在一些问题，比如把数据从内存写到磁盘的时候会阻塞客户端写操作，同时对于主机宕机了，内存中的数据也会丢失，这样就造成数据丢失，因此需要增加一些措施

6.预写日志
避免主机宕机mentable中的数据丢失
解决这个问题的手段就是 WAL（write-ahead log）预写日志技术：在将数据写入 memtable，先通过追加写的方式，将操作记录到处于磁盘的 WAL 当中，这样哪怕宕机导致内存数据丢失，也能通过重放 WAL 的方式，重新恢复 memtable 的数据.此外， WAL 和 memtable 可以建立对应关系，每当一个 memtable 被溢写到磁盘中成为 disktable，其发生数据丢失问题的风险也就随之消除，因此对应的 WAL 也就可以删除了. 并且，由于 WAL 中也是追加写的操作，属于磁盘顺序 IO，因此性能不会成为瓶颈.
//注：这里预写日志采用的是追加写的方式，不会造成性能问题；同时无论是先写wal还是先写mentable都会出现问题，这里是先写wal后写mentable
```

![3](D:\work\项目\KV\图片\3.png)

```java
7.内存读写+只读分层
解决内存向磁盘写数据的时候造成阻塞问题
当 memtable 需要溢写时，就将其变成只读文件，也就是将已有的旧数据归属到 readonly memtable 部分，成为一个只读的数据结构，专注于执行将其溢写到磁盘的流程；于此同时，建立一个全新空白的 active memtable，作为写操作的新入口，这样两个流程之间就实现了解耦，写操作不再需要阻塞.由此可见，active memtable 持有的是最新的数据，readonly memtable 则次之，已经落到磁盘的 disktable 则再次.
//注：这里就像JVM中幸存者区一样，这里面的数据都是有序且唯一的因为采用就绪写的
//注：预写日志中既包含active memtable中的日志，也包含readonly memtable中的数据
```

![4](D:\work\项目\KV\图片\4.png)

```java
8.内存数据结构
memtable底层采用跳表实现
跳表和红黑树对比：使用那种数据结构存储主要是为了解决读性能问题
在读写性能上，跳表和红黑树的性能表现不分伯仲. 但跳表相比于红黑树具有两大核心优势：
• 更简单的实现
• 更细的并发锁粒度
作为存储组件，memtable 中的有序数据不可避免地会被用户并发读写访问，因此需要加锁保证临界资源的安全性和一致性.在锁粒度上，红黑树由于自身染色的机制，每次写操作时都需要对对整棵树的（全量数据）进行加锁，相对笨重；
而跳表则不同，在执行写操作时，可以只针对插入节点局部范围内以及对跳表全局最大高度进行加锁，拥有着更细的锁粒度，在很多场景中是可以做到并发写的.
    
9.磁盘分层
上面我们说了分块存储的思想，解决并发问题阻塞，所有 disktable 是由内存中的 memtable 溢写得到的，这样 disktable 就天然具有两大优势：
• disktable 内部不存在重复 kv 对数据，因为 memtable 执行的是就地写操作
• disktable 内部的 kv 对数据是有序的，因为 memtable 数据本身就是有序的
然而，disktable 间还存在一个局限：
由于每个 memtable 视野有限，只能做到自身范围内的 k 去重和排序. 因此，不同 disktable 之间可能存在重复冗余的 kv 对数据，且不同 disktable 之间的数据无法做到全局有序.
//注：上面的思想就是当内存中的memtable满了之后写到磁盘中，这样memtable的大小和level0中的disktable大小一样大

理清了现状，我们再在此基础之上，进一步引入磁盘 disktable 分层的概念.
• 首先，我们将磁盘整体分为 level0-levelk 共计 k+1层.
• 每个 level 层中的 disktable 数量保持一致
• level(i+1) 中 disktable 的容量大小固定为 level(i) 的 T 倍，T 为常量，通常取值为 10 左右
• 数据流向是由浅入深，层层递进，即由 level(i) -> level(i+1)
• memtable 溢写的数据落到 level0
• levelk 作为兜底
• 当某个 level 内数据总量达到达到阈值时，会发起 level(i) -> level(i+1) 的归并操作
//注：这里的归并操作是把上一层的数据归并到下一层，也就是多个table归并到下一层的一个table
• 数据从 level(i) 流向 level(i+1) 过程中，通过归并操作进行去重和排序，保证 level(i+1) 中 kv 数据无重复且全局有序
结合上述设定，我们可以得出以下结论：
• level0 是特殊的，其中 disktable 之间可能存在冗余的 kv 对数据且不保证全局有序，因为其数据来自 memtable
• level1~levelk 中单层之内没有冗余的 kv 对数据，且保证全局有序
• 不同 level 层之间可能存在冗余的 kv 数据
• 较热（最近写入）的数据位于浅层，较冷（更早写入）的数据位于深层
• levelk 作为最深的一层，整体沉淀的数量达到全局的百分之九十左右

举例：
假设此时 level1 层的数据总量已经达到阈值，接下来需要发起 level(1) -> level(2) 的归并操作：
• 从 level1 中随机选择一个 disktable，尝试将其合并到 level2. 由于数据是有序的，我们可以拿到其中 k 的取值范围. 假设其中最小的 key k_min = 3， 最大的 key k_max = 30，记为[3,30]
• 假设 level2 中有 2 个 disktable 的 k 范围和待合并文件存在重叠，分别为 [0,16] 和 [17,32]
• 将 level1 的 [3,30] 与 level2 的 [0,10] 和 [11,20] 合并，这个过程本质上是个归并排序操作
• 新生成的 disktable [0,30] 不急于插入 level2 ，会根据 level2 中 disktable 的大小规模将其拆分为合适的数量
• 假设拆分得到的两个新的 disktable 分别为 [0,15] 和 [16,30]，将其插入到 level2
• 对应的老数据 level1 的[3,30] 以及 level2 的 [0,10] 和 [11,20] 都被被新数据替代，因此需要删除值得一提的是，倘若因为这一合并操作，导致 level2 的数据容量又超出阈值，则会进一步引起 level 2 到 level 3 的数据合并操作，以此类推，层层递进.
//注：由于下面层的disktable容量要比上面对的大，有可能上面在压缩的过程中多个压缩成一个，或者多个压缩成多个，这里本质也是日志压缩的体现
```

![5](D:\work\项目\KV\图片\5.png)

```java
10.sstable
在 lsm tree 的设定中，对于前文提到的每个磁盘文件块 disktable，设计了一类专门的数据结构 sstable（sorted string table）.
每个 level 的 sstable 容量大约是上一层的10倍，因此一旦到了深层，sstable 的容量可能很大，对应展开的读操作会略显笨重.sstable 在此基础上进行了优化
• 首先 sstable 内部会进一步将 table 拆分为多个 block 块，其在逻辑意义上从属于同一个 sstable；
• 其次，sstable 中会额外维护一个索引信息，其中记录了每个 block 的 k_min 和 k_max 以及每个块中各行的 k_max 和 k_min，便于辅助的查询操作
• 此外，lsm tree 还维护着一个全局索引信息，记录着不同 level 中，每个 sstable 对应的k_max 和 最小 k k_min 的范围
• 最后，每个 sstable 还维护着一个布隆过滤器 bloomfilter，用于快速判断一个 k 是否存在于当前 sstable 中.由于 bloom filter 具有假阳性的特点，因此判定不存在的 k 是必然不存在的，然而判定为存在的 k 也可能存在着一定的失误概率. 有关 bloom filter 的内容，后续我单读写篇文章再展开聊聊，本文先不展开.
    
11.葱兰lsm tree
lsm tree 全称 Log Structure Merge Tree，其核心设定如下：
• 存储介质主要依赖磁盘（sstable），但上层也会借助内存的辅助（memtable）
• 内存（memtable）就地写，磁盘（sstable）顺序写
• 写入口为可读可写的 active memtable
• 达到阈值后 active memtable 转为只读的 readonly memtable
• memtable 保证有序，默认基于跳表实现
• 由于 sstable 来自 memtable，每个sstable 内部无冗余数据且有序
• 磁盘文件分层（level0~levelk），上层为近期写入的热数据，下层为较早写入的冷数据
• level(i+1) sstable 大小恒定为 level(i) 层 T 倍
• level0 sstable 之间存在冗余数据
• level1~levelk 单层内无冗余数据且全局有序
• 数据沿着 level 0 -> level k 的方向合并，自顶向下流动
```

![6](D:\work\项目\KV\图片\6.png)

###### 第三章：读写流程

```java
1.写流程
• 基于就地写模式，写入内存中的 active memtable
• active memtable 达到阈值后转为只读的 readonly memtable
• readonly memtable 会 flush 到磁盘，成为 level0 的 sstable
• level(i) 层数据容量达到后，会基于归并的方式合并到 level(i+1)，以此类推
```

![7](D:\work\项目\KV\图片\7.png)

```java
2.读流程
• 尝试读 active memtable
• 尝试读 readonly memtable
• 尝试读 level0，需要按照溢写顺序进行倒序，依次读 level0 中的每个 sstable（level0 sstable 间数据可能冗余）
• 根据全局的索引文件，依次读 level1~levelk，每个 level 最多只需要读一个 sstable
• 读一个 sstable 时，借助内部的 bloom filter 和索引，加速查询流程
```

![8](D:\work\项目\KV\图片\8.png)



































